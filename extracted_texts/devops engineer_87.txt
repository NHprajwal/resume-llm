JESSICA CLAIRE

100 Montgomery St. 10th Floor
(555) 432-1000 - resumesample@example.com

SUMMARY

e 3 years of experience in the field of DevOps Engineer in application configurations, code compilation, packaging, building, automating,
managing and releasing code from one environment to other and deploying to servers.

e 9+ years of overall experience working in Devops, Big Data and Datawarehouse Technologies.

e Experienced in Jenkins by installing, configuring and maintaining for purpose of continuous integration (CI) and for end to end
automation for all build and deployments and creating Jenkins CI pipelines.

e Hands on experience with EC2, $3, RDS, VPC, ELB, EBS, Auto scaling.

e Experienced in branching, merging and maintaining the versions using SCM tools like Git and GitHub on windows and Linux platform.

e Experienced in Project Management and issue tracking tool like JIRA.

e Experienced in the creation of Docker containers and Docker consoles for managing the application life cycle.

¢ Creating custom Docker Images using Docker file for easier replication of DEV and QA Environments in local machines.

¢ Performed and deployed builds for various Environments like QA, Integration, UAT and Productions Environments Developed and
deployed Chef, puppet based on their cookbooks, recipes and manifest.

¢ Configured and monitored distributed and multi-platform servers using Nagios.

e Strong analytical and problem-solving skills and can work either independently with little or no supervision or as a member of a team.

¢ Good written and verbal communication skills, strong organizational skills and a hard-working team player, well-practiced in attending
phone calls and answering business team queries.

SKILLS
e Languages - SQL, NO SQL
¢ Scripting Language - Shell, Python
* Versioning Tools - Git, GitHub © Web server - Apache HTTP
e CI/ CD - Jenkins Pipeline ¢ Database - MySQL
¢ Build Tools - Maven e Big Data - MongoDB
¢ Ticket Tracking Tool - JIRA * Monitoring Tool - Nagios

e Containerization Tool - Docker, Kubernetes, AWS-ECS/EKS e IAC tool - Terraform
« Operating System - Windows , Linux
e AWS - Amazon EC2, $3, RDS, ELB, EBS, Auto scaling,

CloudFront, Route 53.

EXPERIENCE

01/2015 to Current Devops Engineer
Invitae — Portland, OR
¢ Managed cookbooks in Chef and Implemented environments, roles, and templates in Chef for better environment
management.
© Used Shell scripts to day to day activities and tasks for automating.
e Used Jenkins tool to automate the build process.
e Installing and configuring Jenkins master and slave nodes. Built CI/CD pipeline and managing the infrastructure as
code using chef and puppet.
e Have experience in cloud platform like AWS.
¢ Created and implemented chef cookbooks for deployment and also used Chef Recipes to create a Deployment
directly into Amazon EC2 instances.
e Worked in GIT to manage source code.
¢ Setup Chef Server, workstation, client and wrote scripts to deploy applications.
© Deployed the applications to Tomcat Application Server and static content to Apache web servers.
e Automated the continuous integration and deployments using Jenkins, Docker.
e Installed, Configured, and Managed Monitoring Tools such as Nagios for Resource Monitoring/Network
Monitoring.
¢ Worked on Docker container to create Docker images for different environments.
e Responsible for taking the source code and compiling using Maven and package it in its distributable format, such
as a WAR file.
¢ Implemented process for release management, automated code deployment, configuration management, and
monitoring.
Environment: Amazon EC2, $3, RDS, VPC, ELB, EBS, Auto scaling, UNIX/LINUX, Redhat Linux 6, CentO$§,
Jenkins, Windows, Apache Tomcat, Shell Scripts, Docker, Nagios, puppet.

03/2012 to 01/2015 Big Data / Datawarehouse Tech Lead
Bechtel — San Diego, CA
¢ Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop
clusters with agile methodology.
© Monitored multiple Hadoop clusters environments using Control-M, monitored workload, job performance and
capacity planning using Cloudera Manager.
e Experienced with through hands-on experience in all Hadoop, Java, SQL and Python.
¢ Participated in functional reviews, test specifications and documentation review.
e Performed MapReduce programs on log data to transform into structured way to find user location, age group,
spending time.
e Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit
duration, most purchased product on website.
e Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports by
Business Intelligence tools.
¢ Documented the systems processes and procedures for future references, responsible to manage data coming from
different sources.
¢ Managed multiple projects as a Datawarehouse tech lead in Onsite - Offshore model.
Environment: Hadoop, HDFS, Map Reduce, Flume, Pig, Sqoop, Hive, Pig, Sqoop, Oozie, MongoDB, Shell Scripting.

02/2011 to 03/2012 Systems Engineer
Cognizant Technology Solutions — Tyngsboro, MA
e Supported in Gathering requirement and understanding the business functionality in case of TPR (Technical Project
Request) document.
¢ Develop the graphs according to the business requirements.
e Analyze source systems file layouts and write DML for extracting the data from various sources like flat files,
tables, Mainframe Copy Books. Responder Layouts.
e Involve in analyzing the data transformation logics, mapping implementations and data loading into target database
through Ab-Initio graphs.
¢ Developing UNIX shell scripts for automation processes.
e Involve in fixing the unit and functional test case/data defects.
e Analysis of Existing application and identifying improvements.
Environment -: Ab Initio 3.03, Teradata 12, UNIX (Sun Solaris Korn Shell).

2009 to 02/2011 Ab Initio Developer

Hewlett Packard Global India Pvt Ltd. — City, STATE
© Develop the graphs according to the TPR document.
e Analyze source systems file layouts and write DML for extracting the data from tables.
e Involve in analyzing the data transformation logics, mapping implementations and data loading into target database

through Ab-Initio graphs.

© Developing UNIX shell scripts for automating processes.
e Involve in fixing the unit and functional test case/data defects.
e Analysis of Existing application and identifying improvements.
e Prepare result analysis graphs to validate business test case results.
e Performance testing of all graphs against huge volumes of data.

Environment: Ab Initio 3.03, Teradata 12, UNIX (Sun Solaris Korn Shell), MOSS 2010.

EpucatTIon

Associate in Biology: Natural Sciences
University of Dschang - Dschang, Cameroon

CERTIFICATIONS

e CKA: Certified Kubernetes Administrator.
¢ Red Hat: Certified System Administrator.