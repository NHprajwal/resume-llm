SUMMARY

SKILLS

EXPERIENCE

EDUCATION

Jessica Claire

@ 100 Montgomery St. 10th Floor *2 (555) 432-1000 4 resumesample@example.com

© Automation enthusiast with over eighteen (18) years of experience in Information Technology: just under (5)

years in the role of a DevOps Engineer. Hands-on experience supporting, automating, and optimizing
application deployments in AWS and on-prem. Possess solid experience in software configuration
management, code quality analysis, Cl/CD pipeline creation and maintenance as well as other DevOps
processes.

e« Versioning Tools - Git e Languages - SQL. NO SQL

« Cl- Jenkins « Scripting Language - Shell, Python

« Build Tools - Maven e Web server - Apache Tomcat

« Ticket Tracking Tool - JIRA e Database - SQL Server, MySQL

« Containerization Tool - Docker « Big Data - MongoDB

« Operating System - « Monitoring Tool - Prometheus, Grafana,
Windows , Linux Cloudwatch

e AWS - Amazon EC2, S3, RDS, ELB, EBS, Auto
scaling.

DEVOPS ENGINEER 07/2018 to CURRENT

Radius Ai | Barberton, OH

Environment: Amazon EC2, S3, RDS, VPC, ELB, EBS, Auto scaling, UNIX/LINUX, Redhat Linux 6, CentOs,
Jenkins, Windows, Apache Tomcat, Shell Scripts, Docker.

e M

e Used Shell scripts to day to day activities and tasks for automating.

e Used Jenkins tool to automate the build process.

e Installing and configuring Jenkins master and slave nodes. Built Cl/CD pipeline and managing the
infrastructure as code using chef and puppet.

e Have experience in cloud platform like AWS.

e Created and implemented chef cookbooks for deployment and also used Chef Recipes to create a
Deployment directly into Amazon EC2 instances.

e Worked in GIT to manage source code.

e Setup Chef Server, workstation, client and wrote scripts to deploy applications.

e Deployed the applications to Tomcat Application Server and static content to Apache web servers.

e Automated the continuous integration and deployments using Jenkins, Docker.

e Installed, Configured, and Managed Monitoring Tools such as Nagios for Resource Monitoring/Network
Monitoring.

e Worked on Docker container to create Docker images for different environments.

e Responsible for taking the source code and compiling using Maven and package it in its distributable
format, such as a WAR file.

e Implemented process for release management, automated code deployment, configuration
management, and monitoring.

BI DATA / DATAWAREHOUSE TECH LEAD 03/207 2 to 01/2015
Bwxt | West Bend, WI

e Worked with systems engineering team to plan and deploy new Hadoop environments and expand
existing Hadoop clusters with agile methodology.
e Monitored multiple Hadoop clusters environments using Control-M, monitored workload, job
performance and capacity planning using Cloudera Manager.
e Experienced with through hands-on experience in all Hadoop, Java, SQL and Python.
e Participated in functional reviews, test specifications and documentation review.
e Performed MapReduce programs on log data to transform into structured way to find user location, age
group, spending time.
e Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views,
visit duration, most purchased product on website.
e Exported the analyzed data to the relational databases using Sqoop for visualization and to generate
reports by Business Intelligence tools.
e Documented the systems processes and procedures for future references, responsible to manage data
coming from different sources.
e Managed multiple projects as a Datawarehouse tech lead in Onsite - Offshore model.
Environment: Hadoop. HDFS, Map Reduce, Flume, Pig, Sqoop, Hive, Pig, Sqoop, Oozie, MongoDB, Shell
Scripting.

SYOTEMS ENGINEER 02/2011 to 03/2012
Cognizant Technology Solutions | City, STATE

e Supported in Gathering requirement and understanding the business functionality in case of TPR
(Technical Project Request) document.

e Develop the graphs according to the business requirements.

e Analyze source systems file layouts and write DML for extracting the data from various sources like flat
files, tables, Mainframe Copy Books, Responder Layouts.

e Involve in analyzing the data transformation logics, mapping implementations and data loading into
target database through Ab-Initio graphs.

e Developing UNIX shell scripts for automation processes.

e Involve in fixing the unit and functional test case/data defects.

e Analysis of Existing application and identifying improvements.

Environment: Ab Initio 3.03. Teradata 12, UNIX (Sun Solaris Korn Shell).

AEONITIO DEVELOPER 2009 to 02/2011
Hewlett Packard Global India Pvt Ltd. | City, State

« Develop the graphs according to the TPR document.
e Analyze source systems file layouts and write DML for extracting the data from tables.
« Involve in analyzing the data transformation logics, mapping implementations and data loading into
target database through Ab-Initio graphs.
« Developing UNIX shell scripts for automating processes.
e Involve in fixing the unit and functional test case/data defects.
e Analysis of Existing application and identifying improvements.
e Prepare result analysis graphs to validate business test case results.
e Performance testing of all graphs against huge volumes of data.
Environment: Ab Initio 3.03, Teradata 12, UNIX (Sun Solaris Korn Shell), MOSS 2010.

Bachelor of Science | Information Technology 2020
American InterContinental University