JESSICA CLAIRE

100 Montgomery St. 10th Floor
(555) 432-1000 - resumesample@example.com

SUMMARY

ii Extensive experience in setting up CI/CD pipelines using tools such as Jenkins, Maven, Docker, Nexus, Artifactory and Slack.

ii Extensive experience in distributed Agile and Scrum methodologies to develop best practices for software development and
implementation.

ii Strong experience in migrating applications to AWS platforms.

ii Strong experience in working with version control systems such as Git, GitHub, Bitbucket.

ii Experience working with developing scripts and automation tools used for building, integrating, and deploying software releases to
multiple environments.

ii Good knowledge in scripting languages such as Groovy, YAML Shell scripting.

ii Strong experience in code quality and code coverage tools such as SonarQube, Jacococ.

ii Good knowledge of container technology such as creating Dockerfiles and working with Docker containers.

ii Strong exposure to infrastructure and configuration management tools such as Terraform and Ansible.

ii Good exposure to Kubernetes.

ii Successfully participated in the release cycle of the product which involved environments like Development, QA UAT and Production.
i Worked with project documentation and documented other application related issues, bugs on internal wiki website.

i A highly motivated, energetic individual, a team player with excellent communication and inter-personal skills, has a knack for problem
solving.

SKILLS

- Cloud platforms: AWS - Application/Web Servers: Tomcat, Nginx, Apache

- Framework/DevOps tools: SonarQube, Nexus, Artifactory, - Operating Systems: Ubuntu, Red Hat, Linux, Windows
Jenkins, Slack, JIRA - Databases: SQL Server, MySQL, PostgresSQL

- Build Tools: Maven - Programming Languages: PHP, Java, YAML, Python

- SCMs: Git, GitHub, Bitbucket - Scripting & Other Tools: Git Bash, Shell Script, MobaXterm,
- LAC Tools: Ansible, Terraform Groovy, iTerm

- Containers/Orchestration: Docker, Kubernetes - Monitoring Tools: Datadog

EXPERIENCE

01/2015 to Current Devops Engineer
Invitae — San Diego, CA
e Managed cookbooks in Chef and Implemented environments, roles. and templates in Chef for better environment
management.
e Used Shell scripts to day to day activities and tasks for automating.
© Used Jenkins tool to automate the build process.
e Installing and configuring Jenkins master and slave nodes. Built CI/CD pipeline and managing the infrastructure as
code using chef and puppet.
e Have experience in cloud platform like AWS.
© Created and implemented chef cookbooks for deployment and also used Chef Recipes to create a Deployment
directly into Amazon EC2 instances.
e Worked in GIT to manage source code.
e Setup Chef Server, workstation, client and wrote scripts to deploy applications.
* Deployed the applications to Tomcat Application Server and static content to Apache web servers.
e Automated the continuous integration and deployments using Jenkins, Docker.
e Installed, Configured, and Managed Monitoring Tools such as Nagios for Resource Monitoring/Network
Monitoring.
e Worked on Docker container to create Docker images for different environments.
« Responsible for taking the source code and compiling using Maven and package it in its distributable format, such
as a WAR file.
e Implemented process for release management, automated code deployment, configuration management, and
monitoring.
Environment: Amazon EC2, $3, RDS, VPC, ELB, EBS, Auto scaling, UNIX/LINUX, Redhat Linux 6, CentOS,
Jenkins, Windows, Apache Tomcat, Shell Scripts, Docker, Nagios, puppet.

03/2012 to 01/2015 Big Data / Datawarehouse Tech Lead
Bechtel — Richmond. VA
e Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop
clusters with agile methodology.
Monitored multiple Hadoop clusters environments using Control-M, monitored workload, job performance and
capacity planning using Cloudera Manager.
e Experienced with through hands-on experience in all Hadoop, Java, SQL and Python.
e Participated in functional reviews, test specifications and documentation review.
e Performed MapReduce programs on log data to transform into structured way to find user location, age group,
spending time.
e Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit
duration, most purchased product on website.
e Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports by
Business Intelligence tools.
¢ Documented the systems processes and procedures for future references, responsible to manage data coming from
different sources.
¢ Managed multiple projects as a Datawarehouse tech lead in Onsite - Offshore model.
Environment: Hadoop, HDFS, Map Reduce, Flume, Pig. Sqoop, Hive, Pig. Sqoop, Oozie, MongoDB, Shell Scripting.

02/2011 to 03/2012 Systems Engineer
Cognizant Technology Solutions — Tucson, AZ
¢ Supported in Gathering requirement and understanding the business functionality in case of TPR (Technical Project
Request) document.
¢ Develop the graphs according to the business requirements.
e Analyze source systems file layouts and write DML for extracting the data from various sources like flat files,
tables, Mainframe Copy Books, Responder Layouts.
¢ Involve in analyzing the data transformation logics, mapping implementations and data loading into target database
through Ab-Initio graphs.
© Developing UNIX shell scripts for automation processes.
e Involve in fixing the unit and functional test case/data defects.
e Analysis of Existing application and identifying improvements.
Environment - Ab Initio 3.03, Teradata 12, UNIX (Sun Solaris Korn Shell).

2009 to 02/2011 Ab Initio Developer

Hewlett Packard Global India Pvt Ltd. — City, STATE
e Develop the graphs according to the TPR document.
e Analyze source systems file layouts and write DML for extracting the data from tables.
e Involve in analyzing the data transformation logics, mapping implementations and data loading into target database

through Ab-Initio graphs.

¢ Developing UNIX shell scripts for automating processes.
e Involve in fixing the unit and functional test case/data defects.
e Analysis of Existing application and identifying improvements.
¢ Prepare result analysis graphs to validate business test case results.
e Performance testing of all graphs against huge volumes of data.

Environment: Ab Initio 3.03, Teradata 12, UNIX (Sun Solaris Korn Shell), MOSS 2010.

EpucaTIon

2008 Bachelor of Engineering: Electronics and Communication
Anna University - Chennai, Tamil Nadu

CERTIFICATIONS

e Cloudera Certified Hadoop Developer (CCD - 410).
¢ Certified MongoDB Developer.